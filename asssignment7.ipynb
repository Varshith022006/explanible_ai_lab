{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Varshith022006/explanible_ai_lab/blob/main/asssignment7.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dice-ml\n",
        "!pip install xgboost\n",
        "!pip install shap"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6UCrwpBoOzlz",
        "outputId": "e5a845a2-f1c1-41bb-9d80-2dbe0bdd6f7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting dice-ml\n",
            "  Downloading dice_ml-0.12-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.12/dist-packages (from dice-ml) (4.25.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from dice-ml) (2.0.2)\n",
            "Requirement already satisfied: pandas>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from dice-ml) (2.2.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from dice-ml) (1.6.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from dice-ml) (4.67.1)\n",
            "Collecting raiutils>=0.4.0 (from dice-ml)\n",
            "  Downloading raiutils-0.4.2-py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.12/dist-packages (from dice-ml) (3.1.0)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.12/dist-packages (from dice-ml) (4.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0.0->dice-ml) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0.0->dice-ml) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=2.0.0->dice-ml) (2025.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from raiutils>=0.4.0->dice-ml) (2.32.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from raiutils>=0.4.0->dice-ml) (1.16.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema->dice-ml) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema->dice-ml) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema->dice-ml) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema->dice-ml) (0.27.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->dice-ml) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->dice-ml) (3.6.0)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.12/dist-packages (from xgboost->dice-ml) (2.27.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=2.0.0->dice-ml) (1.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from referencing>=0.28.4->jsonschema->dice-ml) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->raiutils>=0.4.0->dice-ml) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->raiutils>=0.4.0->dice-ml) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->raiutils>=0.4.0->dice-ml) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->raiutils>=0.4.0->dice-ml) (2025.10.5)\n",
            "Downloading dice_ml-0.12-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m32.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading raiutils-0.4.2-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: raiutils, dice-ml\n",
            "Successfully installed dice-ml-0.12 raiutils-0.4.2\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.12/dist-packages (3.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.0.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.27.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from xgboost) (1.16.2)\n",
            "Requirement already satisfied: shap in /usr/local/lib/python3.12/dist-packages (0.49.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from shap) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from shap) (1.16.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from shap) (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from shap) (2.2.2)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.12/dist-packages (from shap) (4.67.1)\n",
            "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.12/dist-packages (from shap) (25.0)\n",
            "Requirement already satisfied: slicer==0.0.8 in /usr/local/lib/python3.12/dist-packages (from shap) (0.0.8)\n",
            "Requirement already satisfied: numba>=0.54 in /usr/local/lib/python3.12/dist-packages (from shap) (0.60.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from shap) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.12/dist-packages (from shap) (4.15.0)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.12/dist-packages (from numba>=0.54->shap) (0.43.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->shap) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->shap) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->shap) (2025.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->shap) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->shap) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->shap) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Njxc-VycOu0_",
        "outputId": "3f3aafc3-f078-4eb5-cdd4-63fbfeb34a23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original shape: (2000, 8)\n",
            "['name', 'city', 'income', 'credit_score', 'loan_amount', 'years_employed', 'points', 'loan_approved']\n",
            "Using target column: loan_approved\n",
            "Target classes mapping: {'False': 0, 'True': 1}\n",
            "\n",
            "Logistic Regression Performance:\n",
            "Accuracy: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "F1-score: 1.0\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       224\n",
            "           1       1.00      1.00      1.00       176\n",
            "\n",
            "    accuracy                           1.00       400\n",
            "   macro avg       1.00      1.00      1.00       400\n",
            "weighted avg       1.00      1.00      1.00       400\n",
            "\n",
            "\n",
            "Random Forest Performance:\n",
            "Accuracy: 1.0\n",
            "Precision: 1.0\n",
            "Recall: 1.0\n",
            "F1-score: 1.0\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00       224\n",
            "           1       1.00      1.00      1.00       176\n",
            "\n",
            "    accuracy                           1.00       400\n",
            "   macro avg       1.00      1.00      1.00       400\n",
            "weighted avg       1.00      1.00      1.00       400\n",
            "\n",
            "\n",
            "Selected instance (Rejected):\n",
            "               name          city  income  credit_score  loan_amount  \\\n",
            "795  James Thompson  Davisborough   62662           466        26460   \n",
            "\n",
            "     years_employed  points loan_approved  \n",
            "795              25    35.0         False  \n",
            "Predicted label: 0 ==> False\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00,  3.22it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Counterfactuals generated:\n",
            "             name            city  income  credit_score  loan_amount  \\\n",
            "0  James Thompson    Davisborough   62662           466        15721   \n",
            "1  James Thompson    Davisborough   62662           467        26460   \n",
            "2  James Thompson  North Jennifer   62662           466        26460   \n",
            "\n",
            "   years_employed  points  loan_approved  \n",
            "0              25    81.4              1  \n",
            "1              25    92.0              1  \n",
            "2              25    67.4              1  \n",
            "\n",
            "=== Loan Decision Status ===\n",
            "Original Instance: False (Rejected)\n",
            "CF_1: True (Approved)\n",
            "CF_2: True (Approved)\n",
            "CF_3: True (Approved)\n",
            "\n",
            "Comparison Table:\n",
            "               example            name            city  income  credit_score  \\\n",
            "0  Original (Rejected)  James Thompson    Davisborough   62662           466   \n",
            "1      CF_1 (Approved)  James Thompson    Davisborough   62662           466   \n",
            "2      CF_2 (Approved)  James Thompson    Davisborough   62662           467   \n",
            "3      CF_3 (Approved)  James Thompson  North Jennifer   62662           466   \n",
            "\n",
            "   loan_amount  years_employed  points loan_approved  \n",
            "0        26460              25    35.0         False  \n",
            "1        15721              25    81.4             1  \n",
            "2        26460              25    92.0             1  \n",
            "3        26460              25    67.4             1  \n",
            "\n",
            "Counterfactuals with Euclidean and Manhattan distances:\n",
            "   loan_approved  euclidean_distance  manhattan_distance            name  \\\n",
            "0              1            2.633231            3.279707  James Thompson   \n",
            "1              1            3.097910            3.104143  James Thompson   \n",
            "2              1            2.258499            3.760914  James Thompson   \n",
            "\n",
            "             city  income  credit_score  loan_amount  years_employed  points  \n",
            "0    Davisborough   62662           466        15721              25    81.4  \n",
            "1    Davisborough   62662           467        26460              25    92.0  \n",
            "2  North Jennifer   62662           466        26460              25    67.4  \n",
            "\n",
            "--- REFLECTIONS ---\n",
            "✔ Original instance was REJECTED.\n",
            "✔ Counterfactuals flipped decision to APPROVED with minimal changes.\n",
            "✔ This shows how small, actionable changes (like income, loan amount, credit history) can alter outcomes.\n",
            "✔ Counterfactuals increase trust by answering 'what-if' questions for end-users.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# -----------------------------\n",
        "# Step 1: Imports\n",
        "# -----------------------------\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "from numpy.linalg import norm\n",
        "\n",
        "# DiCE\n",
        "import dice_ml\n",
        "from dice_ml import Dice\n",
        "\n",
        "# -----------------------------\n",
        "# Step 2: Load dataset\n",
        "# -----------------------------\n",
        "raw_path = \"loan_approval.csv\"\n",
        "df_raw = pd.read_csv(raw_path)\n",
        "print(\"Original shape:\", df_raw.shape)\n",
        "print(df_raw.columns.tolist())\n",
        "# display(df_raw.head()) # Not supported in all environments, will comment out\n",
        "\n",
        "# -----------------------------\n",
        "# Step 3: Clean names & target\n",
        "# -----------------------------\n",
        "df_raw.columns = df_raw.columns.str.lower().str.strip()\n",
        "possible_targets = [c for c in df_raw.columns if ('loan' in c and 'status' in c)]\n",
        "if not possible_targets:\n",
        "    fallback = [c for c in df_raw.columns if c in ['target', 'status', 'label', 'approved']]\n",
        "    possible_targets = fallback\n",
        "if not possible_targets and 'loan_approved' in df_raw.columns: # Add explicit check for 'loan_approved'\n",
        "    possible_targets = ['loan_approved']\n",
        "if not possible_targets:\n",
        "    raise ValueError(\"Could not find target column automatically.\")\n",
        "target_col = possible_targets[0]\n",
        "print(\"Using target column:\", target_col)\n",
        "id_cols = [c for c in df_raw.columns if 'id' in c]\n",
        "if id_cols:\n",
        "    df_raw = df_raw.drop(columns=id_cols)\n",
        "\n",
        "# -----------------------------\n",
        "# Step 4: Missing values\n",
        "# -----------------------------\n",
        "for c in df_raw.columns:\n",
        "    if c != target_col:\n",
        "        df_raw[c] = pd.to_numeric(df_raw[c], errors='ignore')\n",
        "feature_cols = [c for c in df_raw.columns if c != target_col]\n",
        "num_cols = df_raw[feature_cols].select_dtypes(include=[np.number]).columns.tolist()\n",
        "cat_cols = [c for c in feature_cols if c not in num_cols]\n",
        "df_imputed = df_raw.copy()\n",
        "for c in num_cols:\n",
        "    df_imputed[c] = df_imputed[c].fillna(df_imputed[c].median())\n",
        "for c in cat_cols:\n",
        "    mode_val = df_imputed[c].mode(dropna=True)\n",
        "    df_imputed[c] = df_imputed[c].fillna(mode_val[0] if not mode_val.empty else \"missing\")\n",
        "\n",
        "# -----------------------------\n",
        "# Step 5: Encode target (for model training)\n",
        "# -----------------------------\n",
        "le_target = LabelEncoder()\n",
        "y_raw = df_imputed[target_col].astype(str)\n",
        "y_encoded = le_target.fit_transform(y_raw)\n",
        "print(\"Target classes mapping:\", dict(zip(le_target.classes_, range(len(le_target.classes_)))))\n",
        "\n",
        "# -----------------------------\n",
        "# Step 6: Preprocessing pipeline\n",
        "# -----------------------------\n",
        "numeric_transformer = Pipeline(steps=[\n",
        "    ('imputer_num', SimpleImputer(strategy='median')),\n",
        "    ('scaler', StandardScaler())\n",
        "])\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer_cat', SimpleImputer(strategy='most_frequent')),\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "])\n",
        "preprocessor = ColumnTransformer(transformers=[\n",
        "    ('num', numeric_transformer, num_cols),\n",
        "    ('cat', categorical_transformer, cat_cols)\n",
        "])\n",
        "\n",
        "# -----------------------------\n",
        "# Step 7: Train-test split\n",
        "# -----------------------------\n",
        "X = df_imputed[feature_cols].copy()\n",
        "y = y_encoded\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.20, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# -----------------------------\n",
        "# Step 8: Train classifiers\n",
        "# -----------------------------\n",
        "pipe_lr = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                            ('clf', LogisticRegression(max_iter=2000, random_state=42))])\n",
        "pipe_rf = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                            ('clf', RandomForestClassifier(n_estimators=200, random_state=42))])\n",
        "pipe_lr.fit(X_train, y_train)\n",
        "pipe_rf.fit(X_train, y_train) # Corrected from y_test to y_train\n",
        "models = {'Logistic Regression': pipe_lr, 'Random Forest': pipe_rf}\n",
        "for name, model in models.items():\n",
        "    y_pred = model.predict(X_test)\n",
        "    print(f\"\\n{name} Performance:\")\n",
        "    print(\"Accuracy:\", round(accuracy_score(y_test, y_pred), 4))\n",
        "    print(\"Precision:\", round(precision_score(y_test, y_pred, zero_division=0), 4))\n",
        "    print(\"Recall:\", round(recall_score(y_test, y_pred, zero_division=0), 4))\n",
        "    print(\"F1-score:\", round(f1_score(y_test, y_pred, zero_division=0), 4))\n",
        "    print(classification_report(y_test, y_pred, zero_division=0))\n",
        "bb_pipeline = pipe_rf  # final model for DiCE\n",
        "\n",
        "# -----------------------------\n",
        "# Step 9: Wrap data + model (Using df_imputed for DiCE Data)\n",
        "# -----------------------------\n",
        "# Ensure the target column is in a suitable format for DiCE\n",
        "df_imputed[target_col] = df_imputed[target_col].astype('category')\n",
        "df_imputed[target_col] = df_imputed[target_col].fillna(df_imputed[target_col].mode()[0] if not df_imputed[target_col].mode().empty else df_imputed[target_col].iloc[0])\n",
        "\n",
        "\n",
        "d = dice_ml.Data(dataframe=df_imputed, # Use df_imputed which includes the target column\n",
        "                  continuous_features=num_cols, # Use num_cols which are the continuous features\n",
        "                  outcome_name=target_col) # Specify the target column name\n",
        "m = dice_ml.Model(model=bb_pipeline, backend=\"sklearn\", model_type='classifier')\n",
        "exp = Dice(d, m, method=\"random\")\n",
        "\n",
        "# -----------------------------\n",
        "# Step 10: Pick rejected instance\n",
        "# -----------------------------\n",
        "neg_label_encoded = 0  # The encoded label for \"Rejected\"\n",
        "query_instance = None\n",
        "query_instance_original = None # Keep the original query instance for display\n",
        "for idx in X_test.index:\n",
        "    pred = bb_pipeline.predict(X_test.loc[[idx]])[0]\n",
        "    if pred == neg_label_encoded:\n",
        "        query_instance = X_test.loc[[idx]] # Use the instance from X_test (features only)\n",
        "        query_instance_original = df_imputed.loc[[idx]] # Get the original instance from df_imputed\n",
        "        chosen_index = idx\n",
        "        break\n",
        "print(\"\\nSelected instance (Rejected):\")\n",
        "# display(query_instance_original)\n",
        "print(query_instance_original)\n",
        "print(\"Predicted label:\", neg_label_encoded, \"==>\", le_target.inverse_transform([neg_label_encoded])[0])\n",
        "\n",
        "# -----------------------------\n",
        "# Step 11: Generate CFs → Approved\n",
        "# -----------------------------\n",
        "pos_label_encoded = 1 if neg_label_encoded == 0 else 0\n",
        "dice_exp = exp.generate_counterfactuals(query_instance, # Use query_instance (features only) for CF generation\n",
        "                                        total_CFs=3,\n",
        "                                        desired_class=pos_label_encoded,\n",
        "                                        features_to_vary=\"all\")\n",
        "cf_df = dice_exp.cf_examples_list[0].final_cfs_df.reset_index(drop=True)\n",
        "print(\"\\nCounterfactuals generated:\")\n",
        "# display(cf_df)\n",
        "print(cf_df)\n",
        "\n",
        "# -----------------------------\n",
        "# Step 12: Show BEFORE vs AFTER\n",
        "# -----------------------------\n",
        "orig_pred = bb_pipeline.predict(query_instance)[0]\n",
        "cf_preds = bb_pipeline.predict(cf_df[feature_cols])\n",
        "print(\"\\n=== Loan Decision Status ===\")\n",
        "print(\"Original Instance:\", le_target.inverse_transform([orig_pred])[0], \"(Rejected)\")\n",
        "for i, p in enumerate(cf_preds):\n",
        "    print(f\"CF_{i+1}:\", le_target.inverse_transform([p])[0], \"(Approved)\")\n",
        "\n",
        "# Build comparison table (using original query instance)\n",
        "compare_table = pd.concat([\n",
        "    query_instance_original.assign(example=\"Original (Rejected)\"),\n",
        "    cf_df.assign(example=[f\"CF_{i+1} (Approved)\" for i in range(len(cf_df))])\n",
        "], ignore_index=True)\n",
        "cols = ['example'] + [c for c in compare_table.columns if c != 'example']\n",
        "compare_table = compare_table[cols]\n",
        "# display(compare_table)\n",
        "print(\"\\nComparison Table:\")\n",
        "print(compare_table)\n",
        "\n",
        "# -----------------------------\n",
        "# Step 12b: Compute Euclidean and Manhattan distance\n",
        "# -----------------------------\n",
        "# We need to apply the same preprocessor to the original and counterfactual instances\n",
        "X_orig_scaled = preprocessor.transform(query_instance) # Use query_instance (features only)\n",
        "X_cf_scaled = preprocessor.transform(cf_df[feature_cols]) # Use cf_df[feature_cols] (features only)\n",
        "\n",
        "# Compute Euclidean distance (L2 norm) for each counterfactual\n",
        "euclidean_distances = [norm(X_cf_scaled[i] - X_orig_scaled[0]) for i in range(len(cf_df))]\n",
        "cf_df['euclidean_distance'] = euclidean_distances\n",
        "\n",
        "# Compute Manhattan distance (L1 norm) for each counterfactual\n",
        "manhattan_distances = [norm(X_cf_scaled[i] - X_orig_scaled[0], ord=1) for i in range(len(cf_df))]\n",
        "cf_df['manhattan_distance'] = manhattan_distances\n",
        "\n",
        "print(\"\\nCounterfactuals with Euclidean and Manhattan distances:\")\n",
        "# Select the desired columns explicitly from cf_df\n",
        "display_cols = [target_col, 'euclidean_distance', 'manhattan_distance'] + feature_cols\n",
        "# Ensure unique column names in case of any unexpected overlap (though feature_cols should not contain target_col)\n",
        "display_cols = list(dict.fromkeys(display_cols))\n",
        "print(cf_df[display_cols]) # Use the combined and unique list of columns\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# Step 13: Reflections\n",
        "# -----------------------------\n",
        "print(\"\\n--- REFLECTIONS ---\")\n",
        "print(\"✔ Original instance was REJECTED.\")\n",
        "print(\"✔ Counterfactuals flipped decision to APPROVED with minimal changes.\")\n",
        "print(\"✔ This shows how small, actionable changes (like income, loan amount, credit history) can alter outcomes.\")\n",
        "print(\"✔ Counterfactuals increase trust by answering 'what-if' questions for end-users.\")"
      ]
    }
  ]
}